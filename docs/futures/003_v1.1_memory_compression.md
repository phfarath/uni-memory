# Memory Compression

> **Versão Target:** V1.1
> **Status:** ⏳ Pendente
> **Owner:** Unassigned
> **Estimativa:** 80h

---

## Descrição

### Problema
Conforme o Aethera Cortex é usado ao longo do tempo, o banco de dados cresce indefinidamente:

**Cenário real (após 6 meses de uso):**
- Usuário free tier: 800 memórias armazenadas
- Usuário pro tier: 40,000 memórias armazenadas
- Custo de storage: ~$150/mês para 10,000 usuários ativos

**Problemas identificados:**
1. **Memórias antigas ficam irrelevantes** - Contexto de 6 meses atrás raramente é útil
2. **Redundância** - Múltiplas memórias sobre o mesmo tópico
3. **Performance** - Queries de busca ficam lentas com milhões de registros
4. **Custo** - Storage PostgreSQL é caro em escala

**Exemplo do problema:**
```
Memórias antigas (100 registros sobre mesmo projeto):
- "Commit abc123: Fix bug no login"
- "Commit def456: Adiciona validação de email"
- "Commit ghi789: Refactora AuthService"
- ... (97 mais)

Valor para recall hoje: BAIXO (detalhes muito granulares, contexto perdido)
Storage consumido: ~200KB
```

### Solução
Implementar **compressão inteligente de memórias** usando:

1. **Identificação de memórias antigas** (> 90 dias)
2. **Clustering por similaridade** (embeddings + DBSCAN)
3. **Resumo via LLM** - Gera sumário compacto de cluster
4. **Substituição** - Deleta originais, mantém resumo
5. **Retrieval híbrido** - Busca inclui compressed_memories

**Arquitetura:**
```
Memórias antigas (> 90 dias)
    ↓
[Clustering DBSCAN]
    ↓
Clusters temáticos
    ↓
[LLM Summarization]
    ↓
Resumos compactos (1 por cluster)
    ↓
compressed_memories table
    ↓
Deleção de originais
```

**Exemplo do novo fluxo:**
```
ANTES (100 memórias, 200KB):
- "Commit abc123: Fix bug no login"
- "Commit def456: Adiciona validação"
- ... (98 mais)

DEPOIS (1 memória comprimida, 2KB):
- "Período 2025-08 a 2025-11: Refatoração completa do sistema de autenticação.
   Principais mudanças: Migração para OAuth 2.1, implementação de rate limiting,
   adição de testes e2e. Total: 100 commits. Principais contributors: João, Maria."

Redução: 99% de storage
Qualidade: Mantém informação essencial
```

### Valor
✅ **Reduz custos de storage em 70%** após 6 meses de operação
✅ **Melhora performance** - Queries 3x mais rápidas com menos registros
✅ **Mantém contexto histórico** - Resumos preservam informação essencial
✅ **Escalável** - Sistema pode operar indefinidamente sem crescimento linear de DB

**ROI Estimado:**
- Economia de $100k/ano em storage para 50k usuários ativos
- Redução de 60% em latência de retrieval
- Aumento de 30% em precisão (menos ruído nos resultados)

---

## Passos de Implementação

### 1. Código (55h)

#### 1.1 Database Schema (3h)

- [ ] **Adicionar coluna `is_compressed` em `memories`**:
  ```sql
  ALTER TABLE memories
  ADD COLUMN IF NOT EXISTS is_compressed BOOLEAN DEFAULT FALSE;

  CREATE INDEX idx_memories_compressed ON memories(is_compressed, timestamp);
  ```

- [ ] **Criar tabela `compressed_memories`**:
  ```sql
  CREATE TABLE IF NOT EXISTS compressed_memories (
      id SERIAL PRIMARY KEY,
      api_key TEXT REFERENCES api_keys(key),
      session_id TEXT NOT NULL,
      summary TEXT NOT NULL,
      embedding vector(384),
      original_count INTEGER NOT NULL,
      compressed_from_ids INTEGER[],
      time_range_start TIMESTAMPTZ NOT NULL,
      time_range_end TIMESTAMPTZ NOT NULL,
      compression_method TEXT DEFAULT 'llm_summarization',
      metadata JSONB DEFAULT '{}',
      created_at TIMESTAMPTZ DEFAULT NOW()
  );

  CREATE INDEX idx_compressed_session ON compressed_memories(session_id);
  CREATE INDEX idx_compressed_embedding ON compressed_memories
      USING ivfflat (embedding vector_cosine_ops);
  CREATE INDEX idx_compressed_time ON compressed_memories(time_range_start, time_range_end);
  ```

#### 1.2 Memory Compressor Engine (30h)

- [ ] **Criar módulo `app/compression.py`**:
  ```python
  from sklearn.cluster import DBSCAN
  import numpy as np
  from typing import List, Dict
  import logging

  logger = logging.getLogger("Compression")

  class MemoryCompressor:
      """
      Motor de compressão de memórias antigas.

      Responsável por:
      - Identificar memórias antigas (> cutoff_days)
      - Agrupar por similaridade (clustering)
      - Gerar resumos usando LLM
      - Salvar compressed_memories
      - Deletar originais
      """

      def __init__(self, cutoff_days: int = 90):
          self.cutoff_days = cutoff_days
          self.min_cluster_size = 5  # Mínimo de memórias para comprimir
          self.eps = 0.3  # DBSCAN epsilon (distância máxima)

      def identify_old_memories(self, api_key: str) -> List[Dict]:
          """
          Busca memórias antigas para compressão.

          Args:
              api_key: API key do usuário

          Returns:
              Lista de memórias com id, content, timestamp, embedding
          """
          pass

      def cluster_memories(self, memories: List[Dict]) -> List[List[Dict]]:
          """
          Agrupa memórias por similaridade usando DBSCAN.

          Args:
              memories: Lista de memórias com embeddings

          Returns:
              Lista de clusters (cada cluster é lista de memórias)
          """
          pass

      def llm_summarize(self, cluster: List[Dict]) -> str:
          """
          Gera resumo de um cluster usando LLM.

          Args:
              cluster: Lista de memórias similares

          Returns:
              Texto resumido mantendo informação essencial
          """
          pass

      def save_compressed_memory(
          self,
          summary: str,
          cluster: List[Dict],
          api_key: str
      ) -> int:
          """
          Salva memória comprimida no banco.

          Returns:
              compressed_memory_id
          """
          pass

      def delete_originals(self, memory_ids: List[int]):
          """Marca memórias originais como comprimidas e opcionalmente deleta"""
          pass

      def run_compression_job(self, api_key: Optional[str] = None):
          """
          Executa job completo de compressão.

          Se api_key=None, processa todos os usuários.
          """
          pass
  ```

- [ ] **Implementar `identify_old_memories()`**:
  ```python
  def identify_old_memories(self, api_key: str) -> List[Dict]:
      """Busca memórias > cutoff_days que não foram comprimidas"""
      import time

      cutoff_timestamp = time.time() - (self.cutoff_days * 86400)

      conn = get_db_connection()
      c = conn.cursor(cursor_factory=RealDictCursor)

      c.execute("""
          SELECT id, session_id, content, timestamp, embedding
          FROM memories
          WHERE api_key = %s
            AND timestamp < %s
            AND is_compressed = FALSE
          ORDER BY timestamp ASC
      """, (api_key, cutoff_timestamp))

      memories = c.fetchall()
      conn.close()

      logger.info(f"[COMPRESSION] Encontradas {len(memories)} memórias antigas para {api_key}")
      return [dict(m) for m in memories]
  ```

- [ ] **Implementar `cluster_memories()` usando DBSCAN**:
  ```python
  def cluster_memories(self, memories: List[Dict]) -> List[List[Dict]]:
      """Agrupa memórias por similaridade usando DBSCAN"""
      if len(memories) < self.min_cluster_size:
          return []  # Não vale a pena comprimir

      # Extrair embeddings
      embeddings = np.array([m['embedding'] for m in memories])

      # DBSCAN clustering
      clustering = DBSCAN(eps=self.eps, min_samples=self.min_cluster_size, metric='cosine')
      labels = clustering.fit_predict(embeddings)

      # Agrupar memórias por cluster
      clusters = {}
      for idx, label in enumerate(labels):
          if label == -1:  # Noise (outliers)
              continue

          if label not in clusters:
              clusters[label] = []

          clusters[label].append(memories[idx])

      result = list(clusters.values())
      logger.info(f"[COMPRESSION] {len(result)} clusters identificados")

      return result
  ```

- [ ] **Implementar `llm_summarize()`**:
  ```python
  def llm_summarize(self, cluster: List[Dict]) -> str:
      """Gera resumo inteligente usando LLM"""

      # Preparar contexto
      contents = [m['content'] for m in cluster]
      time_range = (
          min(m['timestamp'] for m in cluster),
          max(m['timestamp'] for m in cluster)
      )

      prompt = f"""
      Você é um assistente especializado em resumir históricos de conversação.

      Analise as {len(contents)} memórias abaixo e crie um resumo compacto que:
      1. Preserve informações essenciais (decisões, resultados, pessoas)
      2. Agrupe eventos similares
      3. Mantenha contexto temporal
      4. Máximo de 500 palavras

      Período: {time_range[0]} até {time_range[1]}

      Memórias:
      {"".join(f"- {c}\n" for c in contents[:50])}  # Limitar a 50 primeiras

      Formato do resumo:
      "Período [data]: [Tema principal]. Principais eventos: [lista]. Contributors: [pessoas]. Resultados: [outcomes]."
      """

      response = requests.post(
          "https://api.openai.com/v1/chat/completions",
          headers={"Authorization": f"Bearer {OPENAI_API_KEY}"},
          json={
              "model": "gpt-3.5-turbo",
              "messages": [{"role": "user", "content": prompt}],
              "temperature": 0.3,
              "max_tokens": 800
          },
          timeout=30
      )

      summary = response.json()['choices'][0]['message']['content']
      logger.info(f"[COMPRESSION] Resumo gerado: {len(summary)} chars")

      return summary
  ```

- [ ] **Implementar `save_compressed_memory()`**:
  ```python
  def save_compressed_memory(
      self,
      summary: str,
      cluster: List[Dict],
      api_key: str
  ) -> int:
      """Salva resumo comprimido no banco"""

      # Gerar embedding do resumo
      summary_embedding = embed_model.encode([summary])[0].tolist()

      # Extrair metadados
      session_ids = list(set(m['session_id'] for m in cluster))
      memory_ids = [m['id'] for m in cluster]
      time_range = (
          min(m['timestamp'] for m in cluster),
          max(m['timestamp'] for m in cluster)
      )

      conn = get_db_connection()
      c = conn.cursor()

      c.execute("""
          INSERT INTO compressed_memories (
              api_key,
              session_id,
              summary,
              embedding,
              original_count,
              compressed_from_ids,
              time_range_start,
              time_range_end,
              metadata
          ) VALUES (%s, %s, %s, %s, %s, %s, to_timestamp(%s), to_timestamp(%s), %s)
          RETURNING id
      """, (
          api_key,
          session_ids[0] if len(session_ids) == 1 else 'multi-session',
          summary,
          summary_embedding,
          len(cluster),
          memory_ids,
          time_range[0],
          time_range[1],
          json.dumps({"session_ids": session_ids})
      ))

      compressed_id = c.fetchone()[0]
      conn.commit()
      conn.close()

      logger.info(f"[COMPRESSION] Compressed memory {compressed_id} criada ({len(cluster)} originais)")
      return compressed_id
  ```

- [ ] **Implementar `delete_originals()`**:
  ```python
  def delete_originals(self, memory_ids: List[int]):
      """Marca memórias como comprimidas (soft delete)"""
      conn = get_db_connection()
      c = conn.cursor()

      # Opção 1: Soft delete (mark as compressed)
      c.execute("""
          UPDATE memories
          SET is_compressed = TRUE
          WHERE id = ANY(%s)
      """, (memory_ids,))

      # Opção 2 (alternativa): Hard delete
      # c.execute("DELETE FROM memories WHERE id = ANY(%s)", (memory_ids,))

      affected = c.rowcount
      conn.commit()
      conn.close()

      logger.info(f"[COMPRESSION] {affected} memórias originais marcadas como comprimidas")
  ```

- [ ] **Implementar `run_compression_job()`**:
  ```python
  def run_compression_job(self, api_key: Optional[str] = None):
      """Job principal de compressão"""
      logger.info("[COMPRESSION] Iniciando job de compressão")

      # Se api_key=None, buscar todos usuários
      if api_key is None:
          conn = get_db_connection()
          c = conn.cursor()
          c.execute("SELECT DISTINCT key FROM api_keys WHERE is_active = TRUE")
          api_keys = [row[0] for row in c.fetchall()]
          conn.close()
      else:
          api_keys = [api_key]

      total_compressed = 0
      total_savings = 0

      for key in api_keys:
          # 1. Identificar memórias antigas
          old_memories = self.identify_old_memories(key)

          if len(old_memories) < self.min_cluster_size:
              continue

          # 2. Clustering
          clusters = self.cluster_memories(old_memories)

          # 3. Processar cada cluster
          for cluster in clusters:
              if len(cluster) < self.min_cluster_size:
                  continue

              try:
                  # Gerar resumo
                  summary = self.llm_summarize(cluster)

                  # Salvar compressed memory
                  compressed_id = self.save_compressed_memory(summary, cluster, key)

                  # Deletar originais
                  memory_ids = [m['id'] for m in cluster]
                  self.delete_originals(memory_ids)

                  total_compressed += len(cluster)
                  total_savings += sum(len(m['content']) for m in cluster) - len(summary)

              except Exception as e:
                  logger.error(f"[COMPRESSION] Erro ao processar cluster: {e}")
                  continue

      logger.info(f"[COMPRESSION] Job completo: {total_compressed} memórias comprimidas, {total_savings} bytes economizados")
  ```

#### 1.3 Scheduled Job (8h)

- [ ] **Configurar job semanal em `app/main.py`**:
  ```python
  import schedule

  def scheduled_compression_job():
      """Executado semanalmente aos domingos às 3h AM"""
      try:
          compressor = MemoryCompressor(cutoff_days=90)
          compressor.run_compression_job()
      except Exception as e:
          logger.error(f"[COMPRESSION] Erro no job: {e}")

  # Agendar para domingos às 3h
  schedule.every().sunday.at("03:00").do(scheduled_compression_job)

  @app.on_event("startup")
  async def startup_compression_scheduler():
      import threading

      def run_schedule():
          while True:
              schedule.run_pending()
              time.sleep(3600)  # Check a cada hora

      thread = threading.Thread(target=run_schedule, daemon=True)
      thread.start()
      logger.info("[COMPRESSION] Scheduler iniciado (domingos 3h)")
  ```

#### 1.4 Admin Endpoints (6h)

- [ ] **Adicionar endpoint `POST /admin/compress/run`** (root only):
  ```python
  @app.post("/admin/compress/run")
  async def trigger_compression(
      api_key_target: Optional[str] = None,
      user: dict = Security(verify_api_key)
  ):
      """
      Trigger manual de compressão (root only).

      Body:
          {"api_key_target": "sk_aethera_..." (optional)}

      Se api_key_target=None, processa todos usuários.
      """
      if user['tier'] != 'root':
          raise HTTPException(403, "Root access required")

      compressor = MemoryCompressor()
      compressor.run_compression_job(api_key=api_key_target)

      return {"status": "compression_started"}
  ```

- [ ] **Adicionar endpoint `GET /admin/compress/stats`**:
  ```python
  @app.get("/admin/compress/stats")
  async def get_compression_stats(
      user: dict = Security(verify_api_key)
  ):
      """Retorna estatísticas de compressão"""
      if user['tier'] != 'root':
          raise HTTPException(403, "Root access required")

      conn = get_db_connection()
      c = conn.cursor()

      # Total de compressed memories
      c.execute("SELECT COUNT(*), SUM(original_count) FROM compressed_memories")
      total_compressed, total_originals = c.fetchone()

      # Storage savings estimate
      c.execute("""
          SELECT
              SUM(LENGTH(summary)) as compressed_size,
              (SELECT SUM(LENGTH(content)) FROM memories WHERE is_compressed=TRUE) as original_size
          FROM compressed_memories
      """)
      compressed_size, original_size = c.fetchone()

      conn.close()

      return {
          "total_compressed_memories": total_compressed or 0,
          "total_original_memories_compressed": total_originals or 0,
          "storage_saved_bytes": (original_size or 0) - (compressed_size or 0),
          "compression_ratio": round((compressed_size or 1) / (original_size or 1), 2) if original_size else 0
      }
  ```

#### 1.5 Enhanced Retrieval (8h)

- [ ] **Modificar `retrieve_context_logic()` para incluir compressed_memories**:
  ```python
  def retrieve_context_logic(session_id: str, query: str, limit_k: int = 5) -> List[Dict]:
      """RAG híbrido: memórias regulares + compressed memories"""
      context_items = []
      conn = get_db_connection()
      c = conn.cursor()

      # 1. Short-term (últimas 3, não comprimidas)
      c.execute("""
          SELECT role, content FROM memories
          WHERE session_id = %s AND is_compressed = FALSE
          ORDER BY id DESC LIMIT 3
      """, (session_id,))
      for row in reversed(c.fetchall()):
          context_items.append({
              "source": "short_term",
              "role": row[0],
              "content": row[1]
          })

      # 2. Long-term (regular memories)
      query_vec = embed_model.encode([query])[0].tolist()
      c.execute("""
          SELECT role, content
          FROM memories
          WHERE session_id = %s AND is_compressed = FALSE
          ORDER BY embedding <=> %s
          LIMIT %s
      """, (session_id, query_vec, limit_k))
      for row in c.fetchall():
          context_items.append({
              "source": "long_term",
              "role": row[0],
              "content": row[1]
          })

      # 3. NOVO: Compressed memories (busca por similaridade)
      c.execute("""
          SELECT summary, time_range_start, time_range_end, original_count
          FROM compressed_memories
          WHERE session_id = %s OR metadata->>'session_ids' LIKE %s
          ORDER BY embedding <=> %s
          LIMIT 2
      """, (session_id, f'%{session_id}%', query_vec))

      for row in c.fetchall():
          summary, start, end, count = row
          context_items.append({
              "source": "compressed_memory",
              "role": "assistant",
              "content": f"[Resumo de {count} memórias antigas de {start} até {end}]: {summary}"
          })

      conn.close()
      return context_items
  ```

---

### 2. Testes (15h)

- [ ] **Criar `tests/test_compression.py`**

- [ ] **Test: Identificação de memórias antigas (> 90 dias)**
- [ ] **Test: Clustering de memórias similares**
- [ ] **Test: LLM summarization de cluster**
- [ ] **Test: Salvamento de compressed_memory**
- [ ] **Test: Deleção de memórias originais**
- [ ] **Test: Retrieval incluindo compressed_memories**
- [ ] **Test: Admin endpoint trigger manual**
- [ ] **Test: Stats endpoint**

---

### 3. Documentação (8h)

- [ ] **Atualizar `ARCHITECTURE.md`**: Adicionar seção Memory Compression com diagrama de fluxo
- [ ] **Atualizar `AI_INSTRUCTIONS.md`**: Documentar quando adicionar compression logic
- [ ] **Atualizar `app/README.md`**: Documentar MemoryCompressor API
- [ ] **Atualizar `INLINE_DOCS.md`**: Docstring para `llm_summarize()`

---

### 4. Infraestrutura (2h)

- [ ] **Migration** para adicionar coluna `is_compressed` e tabela `compressed_memories`
- [ ] **Configurar cron job semanal** (schedule library)
- [ ] **Criar índice ivfflat** para embeddings de compressed_memories

---

## Dependências

**Nenhuma** - Feature independente.

---

## Referências

- [Claude-Mem Compression](https://github.com/thedotmack/claude-mem)
- [DBSCAN Clustering](https://scikit-learn.org/stable/modules/clustering.html#dbscan)
- [scikit-learn DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)
- [PostgreSQL Array Types](https://www.postgresql.org/docs/current/arrays.html)

---

## Notas de Implementação

### Estratégia de Compressão
- **Soft delete** preferível a hard delete (permite auditoria)
- **Cutoff padrão: 90 dias** (configurável por tier no futuro)
- **Min cluster size: 5** memórias (menos que isso não compensa)

### Performance
- Job deve rodar em horário de baixo tráfego (domingos 3h AM)
- Processar em batches de 1000 memórias por vez
- Timeout de 30s para chamadas LLM

### Custos
- Cada resumo custa ~$0.002 (GPT-3.5-turbo)
- ROI positivo após comprimir > 50 memórias por cluster
- Economia de storage compensa custo de LLM em 2-3 meses

---

**Versão do documento:** 1.0
**Última atualização:** 2026-02-03
